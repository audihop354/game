{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data_train.iloc[:, :-1].values.astype(np.float32)\n",
    "reviews = data_train.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processingdata(revierws):\n",
    "  reviews_processed = []\n",
    "  for review in reviews:\n",
    "    review_good_one = ''.join([char for char in review if char not in digits])\n",
    "    reviews_processed.append(review_good_one)\n",
    "  word_reviews = []\n",
    "  clean_reviews = []\n",
    "  for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review)\n",
    "  \n",
    "  for statement in word_reviews:\n",
    "    clean = []\n",
    "    for w in statement.split():\n",
    "      new_w = w.translate(str.maketrans('','','!#$%^&*<>?,./:;\"[]{\\}_-+='))\n",
    "      if (new_w!=''):\n",
    "        clean.append(new_w)\n",
    "    clean_reviews.append(clean)\n",
    "  return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pre_processingdata(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == -1:\n",
    "        encoded_labels.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # HOW BIG IS EACH WORD VECTOR\n",
    "MAX_VOCAB_SIZE = 10000 # HOW MANY UNIQUE WORDS TO USE\n",
    "MAX_SEQUENCE_LENGTH = 300 # MAX NUMBER OF WORDS IN A COMMENT TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(data_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (1896, 300)\n",
      "Shape of label train and validation tensor: (1896, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test.iloc[:, :-1].values.astype(np.float32)\n",
    "reviews_test = data_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_test = []\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "encoded_labels_test = np.array(encoded_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_reviews_test = pre_processingdata(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_text = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (501, 2)\n",
      "Shape of label train and validation tensor: (501, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data_test.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5305\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load('Voca-vi.bin')\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "print (vocabulary_size)\n",
    "\n",
    "word_notexist=[]\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors [word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n",
    "        word_notexist.append(word)\n",
    "    \n",
    "del (word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer =Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights =[embedding_matrix],\n",
    "                            trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate \n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 300, 400)     2122000     ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_16 (Reshape)           (None, 300, 1, 400)  0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 298, 1, 100)  120100      ['reshape_16[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 297, 1, 100)  160100      ['reshape_16[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 296, 1, 100)  200100      ['reshape_16[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_19[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_20[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_21[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_15[0][0]',       \n",
      "                                                                  'max_pooling2d_16[0][0]',       \n",
      "                                                                  'max_pooling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 300)          0           ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 300)          0           ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3)            903         ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,603,203\n",
      "Trainable params: 2,603,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5] \n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding =embedding_layer(inputs)\n",
    "reshape=Reshape((sequence_length, 1, EMBEDDING_DIM))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape) \n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape) \n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D ((sequence_length - filter_sizes[0]+1,1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D ((sequence_length - filter_sizes[1]+1,1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D ((sequence_length - filter_sizes[2]+1,1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "cnn_model = Model(inputs, output)\n",
    "adam = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, weight_decay=0.0)\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_Stopping = EarlyStopping(monitor='loss', min_delta=0.1, patience=4, verbose=1)\n",
    "callbacks_list = [early_Stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - ETA: 0s - loss: 4.7263 - categorical_accuracy: 0.9209"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cnn_model\u001b[39m.\u001b[39;49mfit(data, labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(data_test, labels_test),batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49mcallbacks_list, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "cnn_model.fit(data, labels, epochs=5, validation_data=(data_test, labels_test),batch_size=256, callbacks=callbacks_list, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "subplot() takes 1 or 3 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m         ax\u001b[39m.\u001b[39mset_title(metric)\n\u001b[0;32m      8\u001b[0m     plt\u001b[39m.\u001b[39mstyle\u001b[39m.\u001b[39muse(\u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m plot_loss_accuracy(history)\n",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m, in \u001b[0;36mplot_loss_accuracy\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_loss_accuracy\u001b[39m(history):\n\u001b[1;32m----> 2\u001b[0m     fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39;49msubplot(\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m, figsize\u001b[39m=\u001b[39;49m(\u001b[39m10\u001b[39;49m,\u001b[39m4\u001b[39;49m))\n\u001b[0;32m      3\u001b[0m     hist \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m ax, metric \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(axs, [\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:1264\u001b[0m, in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1261\u001b[0m fig \u001b[39m=\u001b[39m gcf()\n\u001b[0;32m   1263\u001b[0m \u001b[39m# First, search for an existing subplot with a matching spec.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m key \u001b[39m=\u001b[39m SubplotSpec\u001b[39m.\u001b[39;49m_from_subplot_args(fig, args)\n\u001b[0;32m   1266\u001b[0m \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m fig\u001b[39m.\u001b[39maxes:\n\u001b[0;32m   1267\u001b[0m     \u001b[39m# if we found an Axes at the position sort out if we can re-use it\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(ax, \u001b[39m'\u001b[39m\u001b[39mget_subplotspec\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m ax\u001b[39m.\u001b[39mget_subplotspec() \u001b[39m==\u001b[39m key:\n\u001b[0;32m   1269\u001b[0m         \u001b[39m# if the user passed no kwargs, re-use\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\gridspec.py:598\u001b[0m, in \u001b[0;36mSubplotSpec._from_subplot_args\u001b[1;34m(figure, args)\u001b[0m\n\u001b[0;32m    596\u001b[0m     rows, cols, num \u001b[39m=\u001b[39m args\n\u001b[0;32m    597\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msubplot() takes 1 or 3 positional arguments but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    599\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m}\u001b[39;00m\u001b[39m were given\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m gs \u001b[39m=\u001b[39m GridSpec\u001b[39m.\u001b[39m_check_gridspec_exists(figure, rows, cols)\n\u001b[0;32m    602\u001b[0m \u001b[39mif\u001b[39;00m gs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: subplot() takes 1 or 3 positional arguments but 2 were given"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    fig, axs = plt.subplot(1,2, figsize=(10,4))\n",
    "    hist = history.history\n",
    "    for ax, metric in zip(axs, [\"loss\",\"accuracy\"]):\n",
    "        ax.plot(hist[metric])\n",
    "        ax.legend([metric])\n",
    "        ax.set_title(metric)\n",
    "    plt.style.use('default')\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cnn_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
