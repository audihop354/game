{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1970, 2)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"Train2.csv\")\n",
    "data_test = pd.read_csv(\"Test2.csv\")\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train.iloc[:, 1].values\n",
    "reviews = data_train.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processingdata(reviews):\n",
    "  reviews_processed = []\n",
    "  for review in reviews:\n",
    "    review_good_one = ''.join([char for char in review if char not in digits])\n",
    "    reviews_processed.append(review_good_one)\n",
    "  word_reviews = []\n",
    "  clean_reviews = []\n",
    "  for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review)\n",
    "  \n",
    "  for statement in word_reviews:\n",
    "    clean = []\n",
    "    for w in statement.split():\n",
    "      new_w = w.translate(str.maketrans('','','!#$%^&*<>?,./:;\"[\"]{\\}_-+='))\n",
    "      if (new_w!=''):\n",
    "        clean.append(new_w)\n",
    "    clean_reviews.append(clean)\n",
    "  return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pre_processingdata(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == -1:\n",
    "        encoded_labels.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # HOW BIG IS EACH WORD VECTOR\n",
    "MAX_VOCAB_SIZE = 10000 # HOW MANY UNIQUE WORDS TO USE\n",
    "MAX_SEQUENCE_LENGTH = 300 # MAX NUMBER OF WORDS IN A COMMENT TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(data_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (5307, 300)\n",
      "Shape of label train and validation tensor: (5307, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test.iloc[:, 1].values\n",
    "reviews_test = data_test.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pre_processingdata(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "encoded_labels_test = []\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "encoded_labels_test = np.array(encoded_labels_test)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "word_reviews_test = pre_processingdata(reviews_test)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (1970, 300)\n",
      "Shape of label train and validation tensor: (1970, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data_test.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels_test.shape)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load('Voca-vi.bin')\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "print (vocabulary_size)\n",
    "\n",
    "word_notexist=[]\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors [word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n",
    "        word_notexist.append(word)\n",
    "  \n",
    "del (word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "print(labels_test)\n",
    "embedding_layer =Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights =[embedding_matrix],\n",
    "                            trainable=True)\n",
    "print(labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate \n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "print(labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 300, 400)     4000000     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 300, 1, 400)  0           ['embedding_1[1][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 298, 1, 100)  120100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 297, 1, 100)  160100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 296, 1, 100)  200100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_6[0][0]',        \n",
      "                                                                  'max_pooling2d_7[0][0]',        \n",
      "                                                                  'max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 300)          0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 300)          0           ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3)            903         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,481,203\n",
      "Trainable params: 4,481,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5] \n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding =embedding_layer(inputs)\n",
    "reshape=Reshape((sequence_length, 1, EMBEDDING_DIM))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape) \n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape) \n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D ((sequence_length - filter_sizes[0]+1,1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D ((sequence_length - filter_sizes[1]+1,1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D ((sequence_length - filter_sizes[2]+1,1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "cnn_model = Model(inputs, output)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, weight_decay=0.0)\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])\n",
    "cnn_model.summary()\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "early_Stopping = EarlyStopping(monitor='loss', min_delta=0.1, patience=4, verbose=1)\n",
    "callbacks_list = [early_Stopping]\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "21/21 - 19s - loss: 6.5873 - categorical_accuracy: 0.5721 - val_loss: 4.9976 - val_categorical_accuracy: 0.7081 - 19s/epoch - 917ms/step\n",
      "Epoch 2/5\n",
      "21/21 - 18s - loss: 4.8937 - categorical_accuracy: 0.7100 - val_loss: 4.1010 - val_categorical_accuracy: 0.7294 - 18s/epoch - 879ms/step\n",
      "Epoch 3/5\n",
      "21/21 - 18s - loss: 3.8521 - categorical_accuracy: 0.7598 - val_loss: 3.4297 - val_categorical_accuracy: 0.7340 - 18s/epoch - 844ms/step\n",
      "Epoch 4/5\n",
      "21/21 - 18s - loss: 3.0903 - categorical_accuracy: 0.8029 - val_loss: 2.9181 - val_categorical_accuracy: 0.7457 - 18s/epoch - 856ms/step\n",
      "Epoch 5/5\n",
      "21/21 - 18s - loss: 2.5313 - categorical_accuracy: 0.8378 - val_loss: 2.4867 - val_categorical_accuracy: 0.7533 - 18s/epoch - 847ms/step\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cnn_model.fit(data, labels, validation_data=(data_test, labels_test), epochs=5 ,batch_size=256, callbacks=callbacks_list, shuffle=True, verbose=2)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 2s 24ms/step\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "prediction = cnn_model.predict(data_test)\n",
    "label_text = ['Tich cuc','Tieu cuc']\n",
    "print(label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test[np.argmax(prediction[1000])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
