{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 2)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train.iloc[:, :-1].values\n",
    "reviews = data_train.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processingdata(reviews):\n",
    "  reviews_processed = []\n",
    "  for review in reviews:\n",
    "    review_good_one = ''.join([char for char in review if char not in digits])\n",
    "    reviews_processed.append(review_good_one)\n",
    "  word_reviews = []\n",
    "  clean_reviews = []\n",
    "  for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review)\n",
    "  \n",
    "  for statement in word_reviews:\n",
    "    clean = []\n",
    "    for w in statement.split():\n",
    "      new_w = w.translate(str.maketrans('','','!#$%^&*<>?,./:;\"[\"]{\\}_-+='))\n",
    "      if (new_w!=''):\n",
    "        clean.append(new_w)\n",
    "    clean_reviews.append(clean)\n",
    "  return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pre_processingdata(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12592\\4279531388.py:3: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if label == \"-1\":\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == \"-1\":\n",
    "        encoded_labels.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # HOW BIG IS EACH WORD VECTOR\n",
    "MAX_VOCAB_SIZE = 10000 # HOW MANY UNIQUE WORDS TO USE\n",
    "MAX_SEQUENCE_LENGTH = 300 # MAX NUMBER OF WORDS IN A COMMENT TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(data_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (1896, 300)\n",
      "Shape of label train and validation tensor: (1896, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test.iloc[:, :-1].values\n",
    "reviews_test = data_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pre_processingdata(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_test = []\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "encoded_labels_test = np.array(encoded_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_reviews_test = pre_processingdata(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (501, 300)\n",
      "Shape of label train and validation tensor: (501, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data_test.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5305\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load('Voca-vi.bin')\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "print (vocabulary_size)\n",
    "\n",
    "word_notexist=[]\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors [word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n",
    "        word_notexist.append(word)\n",
    "    \n",
    "del (word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer =Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights =[embedding_matrix],\n",
    "                            trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate \n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 300, 400)     2122000     ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_12 (Reshape)           (None, 300, 1, 400)  0           ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 298, 1, 100)  120100      ['reshape_12[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 297, 1, 100)  160100      ['reshape_12[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 296, 1, 100)  200100      ['reshape_12[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_18[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_19[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_20 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_20[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_18[0][0]',       \n",
      "                                                                  'max_pooling2d_19[0][0]',       \n",
      "                                                                  'max_pooling2d_20[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 300)          0           ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 300)          0           ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3)            903         ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,603,203\n",
      "Trainable params: 2,603,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5] \n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding =embedding_layer(inputs)\n",
    "reshape=Reshape((sequence_length, 1, EMBEDDING_DIM))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape) \n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape) \n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D ((sequence_length - filter_sizes[0]+1,1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D ((sequence_length - filter_sizes[1]+1,1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D ((sequence_length - filter_sizes[2]+1,1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "cnn_model = Model(inputs, output)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, weight_decay=0.0)\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_Stopping = EarlyStopping(monitor='loss', min_delta=0.1, patience=4, verbose=1)\n",
    "callbacks_list = [early_Stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 - 8s - loss: 5.0539 - categorical_accuracy: 0.8877 - val_loss: 24.1778 - val_categorical_accuracy: 0.3992 - 8s/epoch - 981ms/step\n",
      "Epoch 2/15\n",
      "8/8 - 7s - loss: 4.0843 - categorical_accuracy: 0.9953 - val_loss: 28.4836 - val_categorical_accuracy: 0.3992 - 7s/epoch - 821ms/step\n",
      "Epoch 3/15\n",
      "8/8 - 7s - loss: 3.3250 - categorical_accuracy: 0.9989 - val_loss: 28.8459 - val_categorical_accuracy: 0.3992 - 7s/epoch - 851ms/step\n",
      "Epoch 4/15\n",
      "8/8 - 7s - loss: 2.6389 - categorical_accuracy: 0.9995 - val_loss: 27.5499 - val_categorical_accuracy: 0.3992 - 7s/epoch - 822ms/step\n",
      "Epoch 5/15\n",
      "8/8 - 7s - loss: 2.0669 - categorical_accuracy: 1.0000 - val_loss: 25.5831 - val_categorical_accuracy: 0.3992 - 7s/epoch - 842ms/step\n",
      "Epoch 6/15\n",
      "8/8 - 7s - loss: 1.6134 - categorical_accuracy: 1.0000 - val_loss: 23.4391 - val_categorical_accuracy: 0.3992 - 7s/epoch - 848ms/step\n",
      "Epoch 7/15\n",
      "8/8 - 7s - loss: 1.2619 - categorical_accuracy: 1.0000 - val_loss: 21.3549 - val_categorical_accuracy: 0.3992 - 7s/epoch - 833ms/step\n",
      "Epoch 8/15\n",
      "8/8 - 6s - loss: 0.9948 - categorical_accuracy: 1.0000 - val_loss: 19.4745 - val_categorical_accuracy: 0.3992 - 6s/epoch - 799ms/step\n",
      "Epoch 9/15\n",
      "8/8 - 7s - loss: 0.7910 - categorical_accuracy: 1.0000 - val_loss: 17.7955 - val_categorical_accuracy: 0.3992 - 7s/epoch - 815ms/step\n",
      "Epoch 10/15\n",
      "8/8 - 7s - loss: 0.6366 - categorical_accuracy: 1.0000 - val_loss: 16.3630 - val_categorical_accuracy: 0.3992 - 7s/epoch - 833ms/step\n",
      "Epoch 11/15\n",
      "8/8 - 7s - loss: 0.5183 - categorical_accuracy: 1.0000 - val_loss: 15.1397 - val_categorical_accuracy: 0.3992 - 7s/epoch - 819ms/step\n",
      "Epoch 12/15\n",
      "8/8 - 7s - loss: 0.4268 - categorical_accuracy: 1.0000 - val_loss: 14.0843 - val_categorical_accuracy: 0.3992 - 7s/epoch - 825ms/step\n",
      "Epoch 13/15\n",
      "8/8 - 6s - loss: 0.3559 - categorical_accuracy: 1.0000 - val_loss: 13.1671 - val_categorical_accuracy: 0.3992 - 6s/epoch - 800ms/step\n",
      "Epoch 14/15\n",
      "8/8 - 6s - loss: 0.2997 - categorical_accuracy: 1.0000 - val_loss: 12.4216 - val_categorical_accuracy: 0.3992 - 6s/epoch - 810ms/step\n",
      "Epoch 15/15\n",
      "8/8 - 7s - loss: 0.2554 - categorical_accuracy: 1.0000 - val_loss: 11.7443 - val_categorical_accuracy: 0.3992 - 7s/epoch - 821ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e96bfb39a0>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(data, labels, validation_data=(data_test, labels_test), epochs=15 ,batch_size=256, callbacks=callbacks_list, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = cnn_model.predict(data_test)\n",
    "label_text = ['Tich cuc','Tieu cuc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game hay lắm Đồ họa đẹp nhưng cần tối ưu hơn\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[219], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(reviews_test[\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m label_text[np\u001b[39m.\u001b[39;49margmax(labels_test[\u001b[39m1\u001b[39;49m])]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(reviews_test[1])\n",
    "label_text[np.argmax(labels_test[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test[np.argmax(prediction[2])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
