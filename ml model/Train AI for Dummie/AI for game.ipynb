{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1971, 2)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"Train2.csv\")\n",
    "data_test = pd.read_csv(\"Test2.csv\")\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train.iloc[:, 1].values\n",
    "reviews = data_train.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processingdata(reviews):\n",
    "  reviews_processed = []\n",
    "  for review in reviews:\n",
    "    review_good_one = ''.join([char for char in review if char not in digits])\n",
    "    reviews_processed.append(review_good_one)\n",
    "  word_reviews = []\n",
    "  clean_reviews = []\n",
    "  for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review)\n",
    "  \n",
    "  for statement in word_reviews:\n",
    "    clean = []\n",
    "    for w in statement.split():\n",
    "      new_w = w.translate(str.maketrans('','','!#$%^&*<>?,./:;\"[\"]{\\}_-+='))\n",
    "      if (new_w!=''):\n",
    "        clean.append(new_w)\n",
    "    clean_reviews.append(clean)\n",
    "  return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pre_processingdata(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == -1:\n",
    "        encoded_labels.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # HOW BIG IS EACH WORD VECTOR\n",
    "MAX_VOCAB_SIZE = 10000 # HOW MANY UNIQUE WORDS TO USE\n",
    "MAX_SEQUENCE_LENGTH = 300 # MAX NUMBER OF WORDS IN A COMMENT TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(data_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (5307, 300)\n",
      "Shape of label train and validation tensor: (5307, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test.iloc[:, 1].values\n",
    "reviews_test = data_test.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pre_processingdata(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "encoded_labels_test = []\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "encoded_labels_test = np.array(encoded_labels_test)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "word_reviews_test = pre_processingdata(reviews_test)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (1971, 300)\n",
      "Shape of label train and validation tensor: (1971, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data_test.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels_test.shape)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load('Voca-vi.bin')\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "print (vocabulary_size)\n",
    "\n",
    "word_notexist=[]\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors [word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n",
    "        word_notexist.append(word)\n",
    "  \n",
    "del (word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "print(labels_test)\n",
    "embedding_layer =Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights =[embedding_matrix],\n",
    "                            trainable=True)\n",
    "print(labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate \n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "print(labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 300, 400)     4000000     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 300, 1, 400)  0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 298, 1, 100)  120100      ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 297, 1, 100)  160100      ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 296, 1, 100)  200100      ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_10[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooling2D  (None, 1, 1, 100)   0           ['conv2d_11[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_9[0][0]',        \n",
      "                                                                  'max_pooling2d_10[0][0]',       \n",
      "                                                                  'max_pooling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 300)          0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 300)          0           ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3)            903         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,481,203\n",
      "Trainable params: 4,481,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5] \n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding =embedding_layer(inputs)\n",
    "reshape=Reshape((sequence_length, 1, EMBEDDING_DIM))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape) \n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape) \n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], 1), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D ((sequence_length - filter_sizes[0]+1,1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D ((sequence_length - filter_sizes[1]+1,1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D ((sequence_length - filter_sizes[2]+1,1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "cnn_model = Model(inputs, output)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, weight_decay=0.0)\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])\n",
    "cnn_model.summary()\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "early_Stopping = EarlyStopping(monitor='loss', min_delta=0.1, patience=4, verbose=1)\n",
    "callbacks_list = [early_Stopping]\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "21/21 - 29s - loss: 6.1537 - categorical_accuracy: 0.6017 - val_loss: 4.6486 - val_categorical_accuracy: 0.7164 - 29s/epoch - 1s/step\n",
      "Epoch 2/5\n",
      "21/21 - 26s - loss: 4.4969 - categorical_accuracy: 0.7117 - val_loss: 3.7266 - val_categorical_accuracy: 0.7412 - 26s/epoch - 1s/step\n",
      "Epoch 3/5\n",
      "21/21 - 26s - loss: 3.4309 - categorical_accuracy: 0.7795 - val_loss: 3.0797 - val_categorical_accuracy: 0.7489 - 26s/epoch - 1s/step\n",
      "Epoch 4/5\n",
      "21/21 - 26s - loss: 2.8031 - categorical_accuracy: 0.8020 - val_loss: 2.5864 - val_categorical_accuracy: 0.7595 - 26s/epoch - 1s/step\n",
      "Epoch 5/5\n",
      "21/21 - 26s - loss: 2.3072 - categorical_accuracy: 0.8280 - val_loss: 2.2697 - val_categorical_accuracy: 0.7484 - 26s/epoch - 1s/step\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cnn_model.fit(data, labels, validation_data=(data_test, labels_test), epochs=5 ,batch_size=256, callbacks=callbacks_list, shuffle=True, verbose=2)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 3s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "data_test\n",
    "prediction = cnn_model.predict(data_test)\n",
    "label_text = ['Tich cuc','0', 'Tieu cuc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range (1800,1830):\n",
    "    print(np.argmax(prediction[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
