{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data_train.iloc[:, :-1].values\n",
    "reviews = data_train.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processingdata(revierws):\n",
    "  reviews_processed = []\n",
    "  for review in reviews:\n",
    "    review_good_one = ''.join([char for char in review if char not in digits])\n",
    "    reviews_processed.append(review_good_one)\n",
    "  word_reviews = []\n",
    "  clean_reviews = []\n",
    "  for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review)\n",
    "  \n",
    "  for statement in word_reviews:\n",
    "    clean = []\n",
    "    for w in statement.split():\n",
    "      new_w = w.translate(str.maketrans('','','!#$%^&*<>?,./:;\"[]{\\}_-+='))\n",
    "      if (new_w!=''):\n",
    "        clean.append(new_w)\n",
    "    clean_reviews.append(clean)\n",
    "  return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pre_processingdata(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label == -1:\n",
    "        encoded_labels.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # HOW BIG IS EACH WORD VECTOR\n",
    "MAX_VOCAB_SIZE = 10000 # HOW MANY UNIQUE WORDS TO USE\n",
    "MAX_SEQUENCE_LENGTH = 300 # MAX NUMBER OF WORDS IN A COMMENT TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(data_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (1896, 300)\n",
      "Shape of label train and validation tensor: (1896, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test=data_test.iloc[:, :-1].values\n",
    "reviews_test = data_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_test = []\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "encoded_labels_test = np.array(encoded_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_reviews_test = pre_processingdata(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_text = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train and X validation tensor: (501, 2)\n",
      "Shape of label train and validation tensor: (501, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train and X validation tensor:\", data_test.shape)\n",
    "print(\"Shape of label train and validation tensor:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5305\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load('Voca-vi.bin')\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "print (vocabulary_size)\n",
    "\n",
    "word_notexist=[]\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors [word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n",
    "        word_notexist.append(word)\n",
    "    \n",
    "del (word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer =Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights =[embedding_matrix],\n",
    "                            trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate \n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m embedding \u001b[39m=\u001b[39membedding_layer(inputs)\n\u001b[0;32m      8\u001b[0m reshape\u001b[39m=\u001b[39mReshape((sequence_length, \u001b[39m1\u001b[39m, EMBEDDING_DIM))(embedding)\n\u001b[1;32m---> 10\u001b[0m conv_0 \u001b[39m=\u001b[39m Conv2D(num_filters, (filter_sizes[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, kernel_regularizer\u001b[39m=\u001b[39mregularizers(\u001b[39m0.01\u001b[39;49m))(reshape) \n\u001b[0;32m     11\u001b[0m conv_1 \u001b[39m=\u001b[39m Conv2D(num_filters, (filter_sizes[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, kernel_regularizer\u001b[39m=\u001b[39mregularizers(\u001b[39m0.01\u001b[39m))(reshape) \n\u001b[0;32m     12\u001b[0m conv_2 \u001b[39m=\u001b[39m Conv2D(num_filters, (filter_sizes[\u001b[39m2\u001b[39m], \u001b[39m1\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, kernel_regularizer\u001b[39m=\u001b[39mregularizers(\u001b[39m0.01\u001b[39m))(reshape)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5] \n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding =embedding_layer(inputs)\n",
    "reshape=Reshape((sequence_length, 1, EMBEDDING_DIM))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], 1), activation='relu', kernel_regularizer=regularizers(0.01))(reshape) \n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], 1), activation='relu', kernel_regularizer=regularizers(0.01))(reshape) \n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], 1), activation='relu', kernel_regularizer=regularizers(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D ((sequence_length - filter_size[0]+1,1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D ((sequence_length - filter_size[1]+1,1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D ((sequence_length - filter_size[2]+1,1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers(0.01))(dropout)\n",
    "\n",
    "cnn_model = Model(inputs, output)\n",
    "adam = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
